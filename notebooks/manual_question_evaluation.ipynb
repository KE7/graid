{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Question Applicability Evaluation\n",
    "\n",
    "This notebook allows for manual evaluation of whether questions are applicable to images across different datasets (waymo, nuimages, bdd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from scenic_reasoning.utilities.common import project_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the same random seed as in eval_vlms.py\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths to datasets\n",
    "DB_PATH = project_root_dir() / \"data/databases_ablations\"\n",
    "\n",
    "bdd_path = project_root_dir() / \"data/bdd_val_filtered\"\n",
    "nu_path = project_root_dir() / \"data/nuimages_val_filtered\"\n",
    "waymo_path = project_root_dir() / \"data/waymo_validation_interesting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, sample_size=50):\n",
    "    \"\"\"Load dataset and return sampled dataframes\"\"\"\n",
    "    \n",
    "    if dataset_name.lower() == \"bdd\":\n",
    "        db_base_path = bdd_path\n",
    "        db_path = str(DB_PATH / \"bdd_val_0.2_/co_dino_5scale_swin_l_lsj_16xb1_3x_coco.py.sqlite\")\n",
    "    elif dataset_name.lower() == \"nuimages\":\n",
    "        db_base_path = nu_path\n",
    "        db_path = str(DB_PATH / \"nuimages_val_0.2_/co_dino_5scale_swin_l_lsj_16xb1_3x_coco.py.sqlite\")\n",
    "    elif dataset_name.lower() == \"waymo\":\n",
    "        db_base_path = waymo_path\n",
    "        db_path = str(DB_PATH / \"waymo_validation_0.2_/co_dino_5scale_swin_l_lsj_16xb1_3x_coco.py.sqlite\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Choose from: bdd, nuimages, waymo\")\n",
    "    \n",
    "    print(f\"Loading dataset from {db_path}\")\n",
    "    \n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Get list of all tables\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    tables = pd.read_sql(tables_query, conn)[\"name\"].tolist()\n",
    "    \n",
    "    # Load dataframes\n",
    "    dataframes = {}\n",
    "    for table in tables:\n",
    "        df = pd.read_sql(f\"SELECT * FROM '{table}'\", conn)\n",
    "        dataframes[table] = df\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # Filter and sample dataframes\n",
    "    sampled_dataframes = {}\n",
    "    print(\"Filtering rows...\")\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        filtered_rows = []\n",
    "        for img_idx in tqdm(range(len(df))):\n",
    "            row = df.iloc[img_idx]\n",
    "            d = row.to_dict()\n",
    "            \n",
    "            pkl_path, v = d[\"key\"], json.loads(d[\"value\"])\n",
    "            qa_list = v.get(\"qa_list\", None)\n",
    "            \n",
    "            if not qa_list or qa_list == \"Question not applicable\":\n",
    "                continue\n",
    "                \n",
    "            if isinstance(qa_list[0], list):\n",
    "                qa_list = [random.choice(qa_list)]\n",
    "                \n",
    "            filtered_rows.append(row)\n",
    "            \n",
    "        filtered_df = pd.DataFrame(filtered_rows).reset_index(drop=True)\n",
    "        \n",
    "        # Keep track of available sample size\n",
    "        available_samples = len(filtered_df)\n",
    "        \n",
    "        if available_samples >= sample_size:\n",
    "            sampled_df = filtered_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Table '{table_name}' has only {available_samples} valid rows. Using all.\")\n",
    "            sampled_df = filtered_df.copy()\n",
    "            \n",
    "        sampled_dataframes[table_name] = (sampled_df, available_samples)\n",
    "    \n",
    "    return sampled_dataframes, db_base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(dataset_name, checkpoint_path=None):\n",
    "    \"\"\"Load checkpoint if exists, otherwise return empty data\"\"\"\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = f\"manual_eval_{dataset_name}_checkpoint.json\"\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                print(f\"Loaded checkpoint: Current table: {checkpoint.get('current_table', '')}, \"\n",
    "                      f\"Index: {checkpoint.get('current_idx', 0)}, \"\n",
    "                      f\"Yes count: {checkpoint.get('yes_count', 0)}, \"\n",
    "                      f\"No count: {checkpoint.get('no_count', 0)}\")\n",
    "                return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    \n",
    "    # Return empty checkpoint if file doesn't exist or there was an error\n",
    "    return {\n",
    "        'current_table': '',\n",
    "        'current_idx': 0,\n",
    "        'evaluations': {},\n",
    "        'yes_count': 0,\n",
    "        'no_count': 0,\n",
    "        'history': []\n",
    "    }\n",
    "\n",
    "def save_checkpoint(checkpoint, dataset_name, checkpoint_path=None):\n",
    "    \"\"\"Save checkpoint to resume later\"\"\"\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = f\"manual_eval_{dataset_name}_checkpoint.json\"\n",
    "    \n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(dataset_name, image_data, db_base_path):\n",
    "    \"\"\"Load image based on dataset type\"\"\"\n",
    "    if dataset_name.lower() == \"bdd\":\n",
    "        image_path = image_data[\"name\"]\n",
    "        image_path = str(project_root_dir() / f\"data/bdd100k/images/100k/val/{image_path}\")\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading BDD image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    elif dataset_name.lower() == \"nuimages\":\n",
    "        image_path = image_data[\"filename\"]\n",
    "        image_path = str(project_root_dir() / f\"/home/eecs/liheng/scenic-reasoning/data/nuimages/all/{image_path}\")\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading nuimages image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    elif dataset_name.lower() == \"waymo\":\n",
    "        image_bytes = image_data[\"image\"]\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading waymo image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_questions(dataset_name, sample_size=50, checkpoint_path=None):\n",
    "    \"\"\"Interactive tool to evaluate question applicability for images\"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    sampled_dataframes, db_base_path = load_dataset(dataset_name, sample_size)\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    checkpoint = load_checkpoint(dataset_name, checkpoint_path)\n",
    "    \n",
    "    # Extract checkpoint data\n",
    "    yes_count = checkpoint.get('yes_count', 0)\n",
    "    no_count = checkpoint.get('no_count', 0)\n",
    "    evaluations = checkpoint.get('evaluations', {})\n",
    "    history = checkpoint.get('history', [])\n",
    "    \n",
    "    # Determine starting point\n",
    "    current_table_name = checkpoint.get('current_table', '')\n",
    "    current_idx = checkpoint.get('current_idx', 0)\n",
    "    \n",
    "    # Set up tables to process\n",
    "    table_names = sorted(list(sampled_dataframes.keys()))\n",
    "    \n",
    "    # If we have a checkpoint but the table is missing, start from beginning\n",
    "    if current_table_name and current_table_name not in table_names:\n",
    "        print(f\"Warning: Table {current_table_name} from checkpoint not found. Starting from beginning.\")\n",
    "        current_table_name = table_names[0] if table_names else ''\n",
    "        current_idx = 0\n",
    "    \n",
    "    # If no table is specified in checkpoint, start with the first one\n",
    "    if not current_table_name and table_names:\n",
    "        current_table_name = table_names[0]\n",
    "    \n",
    "    # If there are no tables at all, exit\n",
    "    if not table_names:\n",
    "        print(\"No tables found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Start table index at the current table\n",
    "    table_idx = table_names.index(current_table_name) if current_table_name in table_names else 0\n",
    "    \n",
    "    print(f\"\\nStarting evaluation from table {current_table_name}, index {current_idx}\")\n",
    "    print(f\"Current counts - Yes: {yes_count}, No: {no_count}\")\n",
    "    \n",
    "    # Main evaluation loop\n",
    "    try:\n",
    "        # Loop through tables\n",
    "        while table_idx < len(table_names):\n",
    "            table_name = table_names[table_idx]\n",
    "            sampled_df, available_samples = sampled_dataframes[table_name]\n",
    "            \n",
    "            if table_name != current_table_name:\n",
    "                current_idx = 0\n",
    "                current_table_name = table_name\n",
    "            \n",
    "            # Loop through samples in the current table\n",
    "            while current_idx < len(sampled_df):\n",
    "                # Load row data\n",
    "                row = sampled_df.iloc[current_idx]\n",
    "                d = row.to_dict()\n",
    "                \n",
    "                pkl_path, v = d[\"key\"], json.loads(d[\"value\"])\n",
    "                pkl_path = str(db_base_path / pkl_path)\n",
    "                \n",
    "                qa_list = v[\"qa_list\"]\n",
    "                if not qa_list or qa_list == \"Question not applicable\":\n",
    "                    current_idx += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load image data\n",
    "                try:\n",
    "                    with open(pkl_path, \"rb\") as f:\n",
    "                        image_data = pickle.load(f)\n",
    "                    \n",
    "                    # Load the actual image\n",
    "                    image = load_image(dataset_name, image_data, db_base_path)\n",
    "                    \n",
    "                    # Extract question and answer\n",
    "                    if isinstance(qa_list[0], list):\n",
    "                        random_qa = random.choice(qa_list)\n",
    "                        question = random_qa[0]\n",
    "                        answer = random_qa[1]\n",
    "                    else:\n",
    "                        question = qa_list[0]\n",
    "                        answer = qa_list[1]\n",
    "                    \n",
    "                    # Generate a unique ID for this evaluation\n",
    "                    eval_id = f\"{table_name}_{current_idx}\"\n",
    "                    \n",
    "                    # Skip if already evaluated (unless we're revisiting history)\n",
    "                    if eval_id in evaluations and len(history) == 0:\n",
    "                        print(f\"Skipping already evaluated {eval_id}\")\n",
    "                        current_idx += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Display the image, question and answer\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"\\nTable: {table_name} | Sample: {current_idx+1}/{len(sampled_df)} | Yes: {yes_count}, No: {no_count}\")\n",
    "                    print(f\"\\nQuestion: {question}\")\n",
    "                    print(f\"Answer: {answer}\")\n",
    "                    \n",
    "                    if image is not None:\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        plt.imshow(np.array(image))\n",
    "                        plt.axis('off')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        print(\"[Error: Could not load image]\")\n",
    "                    \n",
    "                    # Get user input\n",
    "                    user_input = input(\"\\nIs the question applicable to this image? (yes/no/skip/previous/quit): \").lower().strip()\n",
    "                    \n",
    "                    if user_input == 'quit' or user_input == 'q':\n",
    "                        # Save checkpoint before quitting\n",
    "                        checkpoint = {\n",
    "                            'current_table': table_name,\n",
    "                            'current_idx': current_idx,\n",
    "                            'evaluations': evaluations,\n",
    "                            'yes_count': yes_count,\n",
    "                            'no_count': no_count,\n",
    "                            'history': history\n",
    "                        }\n",
    "                        save_checkpoint(checkpoint, dataset_name, checkpoint_path)\n",
    "                        print(\"\\nEvaluation paused. Run the notebook again to continue.\")\n",
    "                        return\n",
    "                    \n",
    "                    elif user_input == 'previous' or user_input == 'p':\n",
    "                        # Go back to the previous evaluation\n",
    "                        if len(history) > 0:\n",
    "                            prev = history.pop()\n",
    "                            table_idx = table_names.index(prev['table'])\n",
    "                            current_idx = prev['idx']\n",
    "                            current_table_name = prev['table']\n",
    "                            \n",
    "                            # Adjust counts if needed\n",
    "                            prev_eval_id = f\"{prev['table']}_{prev['idx']}\"\n",
    "                            if prev_eval_id in evaluations:\n",
    "                                if evaluations[prev_eval_id] == 'yes':\n",
    "                                    yes_count -= 1\n",
    "                                elif evaluations[prev_eval_id] == 'no':\n",
    "                                    no_count -= 1\n",
    "                                del evaluations[prev_eval_id]\n",
    "                            \n",
    "                            continue\n",
    "                        else:\n",
    "                            print(\"No previous evaluation to go back to.\")\n",
    "                            continue\n",
    "                    \n",
    "                    elif user_input == 'skip' or user_input == 's':\n",
    "                        # Add to history before moving on\n",
    "                        history.append({\n",
    "                            'table': table_name,\n",
    "                            'idx': current_idx\n",
    "                        })\n",
    "                        current_idx += 1\n",
    "                        continue\n",
    "                    \n",
    "                    elif user_input in ['yes', 'y']:\n",
    "                        # Record the evaluation\n",
    "                        evaluations[eval_id] = 'yes'\n",
    "                        yes_count += 1\n",
    "                    \n",
    "                    elif user_input in ['no', 'n']:\n",
    "                        # Record the evaluation\n",
    "                        evaluations[eval_id] = 'no'\n",
    "                        no_count += 1\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Invalid input. Please enter yes, no, skip, previous, or quit.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add to history before moving on\n",
    "                    history.append({\n",
    "                        'table': table_name,\n",
    "                        'idx': current_idx\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint periodically (every 5 evaluations)\n",
    "                    if (yes_count + no_count) % 5 == 0:\n",
    "                        checkpoint = {\n",
    "                            'current_table': table_name,\n",
    "                            'current_idx': current_idx + 1,  # Save the next index to start from\n",
    "                            'evaluations': evaluations,\n",
    "                            'yes_count': yes_count,\n",
    "                            'no_count': no_count,\n",
    "                            'history': history\n",
    "                        }\n",
    "                        save_checkpoint(checkpoint, dataset_name, checkpoint_path)\n",
    "                    \n",
    "                    # Move to next sample\n",
    "                    current_idx += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample {current_idx} in table {table_name}: {e}\")\n",
    "                    current_idx += 1\n",
    "            \n",
    "            # Move to next table\n",
    "            table_idx += 1\n",
    "            current_idx = 0\n",
    "        \n",
    "        # Evaluation complete\n",
    "        clear_output(wait=True)\n",
    "        print(\"\\nEvaluation complete!\")\n",
    "        print(f\"Final counts - Yes: {yes_count}, No: {no_count}\")\n",
    "        print(f\"Applicability rate: {yes_count / (yes_count + no_count) * 100:.2f}%\")\n",
    "        \n",
    "        # Save final results\n",
    "        results = {\n",
    "            'dataset': dataset_name,\n",
    "            'evaluations': evaluations,\n",
    "            'yes_count': yes_count,\n",
    "            'no_count': no_count,\n",
    "            'applicability_rate': yes_count / (yes_count + no_count) if (yes_count + no_count) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        results_path = f\"manual_eval_{dataset_name}_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {results_path}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        # Handle interruption gracefully\n",
    "        print(\"\\nEvaluation interrupted.\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'current_table': current_table_name,\n",
    "            'current_idx': current_idx,\n",
    "            'evaluations': evaluations,\n",
    "            'yes_count': yes_count,\n",
    "            'no_count': no_count,\n",
    "            'history': history\n",
    "        }\n",
    "        save_checkpoint(checkpoint, dataset_name, checkpoint_path)\n",
    "        print(\"Checkpoint saved. Run the notebook again to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Manual Evaluation\n",
    "\n",
    "Execute the cell below to start the evaluation. You'll need to specify:\n",
    "1. The dataset to evaluate (waymo, nuimages, or bdd)\n",
    "2. The sample size (number of samples per table)\n",
    "3. Optionally, a custom checkpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /work/ke/research/scenic-reasoning/data/databases_ablations/waymo_validation_0.2_/co_dino_5scale_swin_l_lsj_16xb1_3x_coco.py.sqlite\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Optional: Specify a custom checkpoint path\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mevaluate_questions\u001b[0;34m(dataset_name, sample_size, checkpoint_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Interactive tool to evaluate question applicability for images\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sampled_dataframes, db_base_path \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load checkpoint if exists\u001b[39;00m\n\u001b[1;32m      8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m load_checkpoint(dataset_name, checkpoint_path)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset_name, sample_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Connect to SQLite database\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get list of all tables\u001b[39;00m\n\u001b[1;32m     22\u001b[0m tables_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT name FROM sqlite_master WHERE type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "dataset_name = \"waymo\"  # Options: \"waymo\", \"nuimages\", \"bdd\"\n",
    "sample_size = 100  # Number of samples per table\n",
    "checkpoint_path = None  # Optional: Specify a custom checkpoint path\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_questions(dataset_name, sample_size, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "After completing the evaluation or if you want to check your progress, run the cell below to view the current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_results(dataset_name):\n",
    "    \"\"\"View evaluation results\"\"\"\n",
    "    results_path = f\"manual_eval_{dataset_name}_results.json\"\n",
    "    checkpoint_path = f\"manual_eval_{dataset_name}_checkpoint.json\"\n",
    "    \n",
    "    # Try results file first\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(f\"Results for {dataset_name} dataset:\")\n",
    "        print(f\"Yes count: {results['yes_count']}\")\n",
    "        print(f\"No count: {results['no_count']}\")\n",
    "        print(f\"Applicability rate: {results['applicability_rate']*100:.2f}%\")\n",
    "        \n",
    "    # If no results file, check checkpoint\n",
    "    elif os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        \n",
    "        print(f\"Checkpoint for {dataset_name} dataset:\")\n",
    "        print(f\"Current table: {checkpoint.get('current_table', '')}\")\n",
    "        print(f\"Current index: {checkpoint.get('current_idx', 0)}\")\n",
    "        print(f\"Yes count: {checkpoint.get('yes_count', 0)}\")\n",
    "        print(f\"No count: {checkpoint.get('no_count', 0)}\")\n",
    "        \n",
    "        if checkpoint.get('yes_count', 0) + checkpoint.get('no_count', 0) > 0:\n",
    "            applicability_rate = checkpoint.get('yes_count', 0) / (checkpoint.get('yes_count', 0) + checkpoint.get('no_count', 0))\n",
    "            print(f\"Current applicability rate: {applicability_rate*100:.2f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"No results or checkpoint found for {dataset_name} dataset.\")\n",
    "\n",
    "# Replace with your dataset name\n",
    "view_results(\"waymo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenic_reason",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
