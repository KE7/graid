{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinchon/miniconda3/envs/detectron2_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from scenic_reasoning.measurements.ObjectDetection import ObjectDetectionMeasurements\n",
    "from scenic_reasoning.utilities.common import get_default_device\n",
    "import torch\n",
    "from ultralytics.data.augment import LetterBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_TO_SHOW = 1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_transform = LetterBox(new_shape=(720, 1280))\n",
    "def transform_image_for_yolo(image : torch.Tensor):\n",
    "    # 1) convert from tensor to cv2 image\n",
    "    image_np  = image.permute(1, 2, 0).numpy()\n",
    "    # 2) resize to 768x1280\n",
    "    image_np = shape_transform(image=image_np)\n",
    "    # 3) convert back to tensor\n",
    "    image = torch.tensor(image_np).permute(2, 0, 1)\n",
    "    # 4) normalize to 0-1\n",
    "    image = image.to(torch.float32) / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scenic_reasoning.data.ImageLoader.Bdd100kDataset object at 0x383392600>\n",
      "10000\n",
      "torch.Size([3, 720, 1280])\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "from scenic_reasoning.data.ImageLoader import Bdd100kDataset\n",
    "\n",
    "'''\n",
    "This 'transform_image_for_yolo' was isolated to be problematic with Bdd100kDataset,\n",
    "which is understandable give this transform fn was originally intended for YOLO format\n",
    "We take the original 720x1280 image input resolution to do proper inference upon\n",
    "'''\n",
    "'''\n",
    "bdd = Bdd100kDataset(\n",
    "    split=\"val\", \n",
    "    # YOLO requires images to be 640x640 or 768x1280, \n",
    "    # but BDD100K images are 720x1280 so we need to resize\n",
    "    transform=transform_image_for_yolo,  \n",
    "    use_original_categories=False,\n",
    "    use_extended_annotations=False,\n",
    ")\n",
    "'''\n",
    "\n",
    "bdd = Bdd100kDataset(\n",
    "    split=\"val\",\n",
    "    use_original_categories=False,\n",
    "    use_extended_annotations=False)\n",
    "\n",
    "print(bdd)\n",
    "print(len(bdd))\n",
    "first_img = bdd[0]\n",
    "first_img_tensor = first_img['image']\n",
    "print(first_img_tensor.shape)\n",
    "print(len(first_img['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.models.UltralyticsYolo import Yolo\n",
    "\n",
    "# https://docs.ultralytics.com/models/yolov5/#performance-metrics\n",
    "# model = Yolo(model=\"../yolov5x6u.pt\") # v5 can handle 1280 while v8 can handle 640. makes no sense ><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.models.Detectron import Detectron2Model\n",
    "\n",
    "threshold = 0.5\n",
    "config_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "weights_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "model = Detectron2Model(\n",
    "    config_file=config_file, \n",
    "    weights_file=weights_file, \n",
    "    threshold=threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_img_tensor dims torch.Size([3, 720, 1280])\n",
      "image should be CHW: torch.Size([3, 720, 1280])\n",
      "image should be HWC: (720, 1280, 3)\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=18, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[ 831.0070,  322.6685, 1206.9893,  521.2864],\n",
      "        [ 765.7220,  303.8628,  938.9324,  441.8760],\n",
      "        [ 732.2385,  319.1121,  792.3572,  390.5504],\n",
      "        [ 687.2977,  331.2784,  711.5379,  353.2328],\n",
      "        [ 457.9689,  265.3659,  590.2451,  410.5630],\n",
      "        [ 714.3595,  328.7690,  743.0000,  370.7492],\n",
      "        [ 248.7974,  332.9140,  343.5569,  391.2460],\n",
      "        [ 602.7080,  336.1413,  626.0717,  352.5395],\n",
      "        [ 163.8347,  342.5507,  266.0285,  403.6561],\n",
      "        [  33.5951,  332.2769,  246.5630,  414.9995],\n",
      "        [ 386.9363,  338.2810,  445.6151,  375.8312],\n",
      "        [ 670.0027,  333.3142,  689.0413,  344.9504],\n",
      "        [ 753.6214,  321.5774,  796.9987,  415.9343],\n",
      "        [ 583.1193,  333.8482,  601.8462,  347.5010],\n",
      "        [ 729.9250,  328.8670,  754.0770,  387.9754],\n",
      "        [ 709.8313,  328.8125,  730.6232,  361.2158],\n",
      "        [ 310.4894,  339.4500,  375.1166,  384.7361],\n",
      "        [ 328.7814,  325.6616,  380.1366,  380.5633]])), scores: tensor([0.9969, 0.9883, 0.9700, 0.9600, 0.9585, 0.9563, 0.9100, 0.9034, 0.8433, 0.6903, 0.6607, 0.6581, 0.6553, 0.6477, 0.6251, 0.6165, 0.5815, 0.5549]), pred_classes: tensor([2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7])])}\n",
      "---------------------------------\n",
      "torch.Size([3, 720, 1280])\n",
      "image should be CHW: torch.Size([3, 720, 1280])\n",
      "image should be HWC: (720, 1280, 3)\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=19, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[6.8363e+02, 3.5803e+02, 7.1776e+02, 3.9149e+02],\n",
      "        [3.9739e-01, 3.4041e+02, 5.2140e+01, 4.0087e+02],\n",
      "        [2.4966e+02, 3.4410e+02, 3.4852e+02, 3.9540e+02],\n",
      "        [7.9386e+02, 3.6018e+02, 9.0129e+02, 4.2734e+02],\n",
      "        [4.5918e+01, 3.4805e+02, 1.2667e+02, 3.9822e+02],\n",
      "        [7.2831e+02, 3.6429e+02, 7.5923e+02, 4.0362e+02],\n",
      "        [1.2017e+03, 3.9434e+02, 1.2791e+03, 5.2392e+02],\n",
      "        [8.8209e+02, 3.7626e+02, 9.5036e+02, 4.4668e+02],\n",
      "        [7.5011e+02, 3.5995e+02, 7.8137e+02, 4.0815e+02],\n",
      "        [7.5801e+02, 3.6191e+02, 8.0814e+02, 4.1125e+02],\n",
      "        [9.3557e+02, 3.3723e+02, 1.2033e+03, 4.8237e+02],\n",
      "        [7.0548e+02, 3.6375e+02, 7.3333e+02, 3.9648e+02],\n",
      "        [2.0293e+02, 3.3985e+02, 2.5388e+02, 3.8553e+02],\n",
      "        [2.0583e+02, 3.3887e+02, 3.0272e+02, 3.8574e+02],\n",
      "        [4.4592e+02, 3.5903e+02, 4.6181e+02, 3.7258e+02],\n",
      "        [5.9609e+02, 3.5714e+02, 6.2970e+02, 3.8082e+02],\n",
      "        [5.0162e+02, 3.5794e+02, 5.1712e+02, 3.7152e+02],\n",
      "        [5.3377e+02, 3.3580e+02, 6.0762e+02, 3.6718e+02],\n",
      "        [6.5214e+02, 3.5759e+02, 6.6263e+02, 3.8722e+02]])), scores: tensor([0.9841, 0.9774, 0.9734, 0.9564, 0.9315, 0.9061, 0.8895, 0.8733, 0.8522, 0.8495, 0.8495, 0.8266, 0.8115, 0.6523, 0.6248, 0.6134, 0.6039, 0.5260, 0.5035]), pred_classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 5, 0])])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x35be1cd10>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x349e23ce0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x37766c770>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8cd40>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8cf50>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d040>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8cda0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8ce60>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d160>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d100>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d250>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d1f0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d3a0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d460>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d520>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d5e0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d6a0>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d760>,\n",
       " <scenic_reasoning.interfaces.ObjectDetectionI.ObjectDetectionResultI at 0x36cb8d820>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "# Tested with local raw image path for Detectron2 inference upon\n",
    "sample_img_path = \"/Users/kevinchon/Documents/KE7/scenic-reasoning/data/bdd100k/images/100k/val/b1c9c847-3bda4659.jpg\"\n",
    "sample_img = Image.open(sample_img_path)\n",
    "sample_img_tensor = decode_image(sample_img_path)\n",
    "print(f\"sample_img_tensor dims {sample_img_tensor.shape}\")\n",
    "model.identify_for_image(sample_img_tensor)\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "# Tested with the specific individual image of BDD Dataset\n",
    "# Able to output of proper confidence scores and classification labels\n",
    "first_img = bdd[0]\n",
    "first_img_tensor = first_img['image']\n",
    "print(first_img_tensor.shape)\n",
    "\n",
    "model.identify_for_image(first_img_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=19, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[6.8363e+02, 3.5803e+02, 7.1776e+02, 3.9149e+02],\n",
      "        [3.9739e-01, 3.4041e+02, 5.2140e+01, 4.0087e+02],\n",
      "        [2.4966e+02, 3.4410e+02, 3.4852e+02, 3.9540e+02],\n",
      "        [7.9386e+02, 3.6018e+02, 9.0129e+02, 4.2734e+02],\n",
      "        [4.5918e+01, 3.4805e+02, 1.2667e+02, 3.9822e+02],\n",
      "        [7.2831e+02, 3.6429e+02, 7.5923e+02, 4.0362e+02],\n",
      "        [1.2017e+03, 3.9434e+02, 1.2791e+03, 5.2392e+02],\n",
      "        [8.8209e+02, 3.7626e+02, 9.5036e+02, 4.4668e+02],\n",
      "        [7.5011e+02, 3.5995e+02, 7.8137e+02, 4.0815e+02],\n",
      "        [7.5801e+02, 3.6191e+02, 8.0814e+02, 4.1125e+02],\n",
      "        [9.3557e+02, 3.3723e+02, 1.2033e+03, 4.8237e+02],\n",
      "        [7.0548e+02, 3.6375e+02, 7.3333e+02, 3.9648e+02],\n",
      "        [2.0293e+02, 3.3985e+02, 2.5388e+02, 3.8553e+02],\n",
      "        [2.0583e+02, 3.3887e+02, 3.0272e+02, 3.8574e+02],\n",
      "        [4.4592e+02, 3.5903e+02, 4.6181e+02, 3.7258e+02],\n",
      "        [5.9609e+02, 3.5714e+02, 6.2970e+02, 3.8082e+02],\n",
      "        [5.0162e+02, 3.5794e+02, 5.1712e+02, 3.7152e+02],\n",
      "        [5.3377e+02, 3.3580e+02, 6.0762e+02, 3.6718e+02],\n",
      "        [6.5214e+02, 3.5759e+02, 6.6263e+02, 3.8722e+02]])), scores: tensor([0.9841, 0.9774, 0.9734, 0.9564, 0.9315, 0.9061, 0.8895, 0.8733, 0.8522, 0.8495, 0.8495, 0.8266, 0.8115, 0.6523, 0.6248, 0.6134, 0.6039, 0.5260, 0.5035]), pred_classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 5, 0])])}\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=0, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([], size=(0, 4))), scores: tensor([]), pred_classes: tensor([], dtype=torch.int64)])}\n",
      "No instances or predictions in this image.\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=4, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[355.8690, 266.4515, 445.4924, 326.6559],\n",
      "        [610.4634, 279.0677, 647.8254, 304.9223],\n",
      "        [478.1564, 270.8881, 526.0388, 292.0101],\n",
      "        [703.4118, 280.1898, 727.0935, 292.9653]])), scores: tensor([0.9870, 0.9786, 0.7939, 0.6213]), pred_classes: tensor([2, 2, 2, 2])])}\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=18, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[ 831.0070,  322.6685, 1206.9893,  521.2864],\n",
      "        [ 765.7220,  303.8628,  938.9324,  441.8760],\n",
      "        [ 732.2385,  319.1121,  792.3572,  390.5504],\n",
      "        [ 687.2977,  331.2784,  711.5379,  353.2328],\n",
      "        [ 457.9689,  265.3659,  590.2451,  410.5630],\n",
      "        [ 714.3595,  328.7690,  743.0000,  370.7492],\n",
      "        [ 248.7974,  332.9140,  343.5569,  391.2460],\n",
      "        [ 602.7080,  336.1413,  626.0717,  352.5395],\n",
      "        [ 163.8347,  342.5507,  266.0285,  403.6561],\n",
      "        [  33.5951,  332.2769,  246.5630,  414.9995],\n",
      "        [ 386.9363,  338.2810,  445.6151,  375.8312],\n",
      "        [ 670.0027,  333.3142,  689.0413,  344.9504],\n",
      "        [ 753.6214,  321.5774,  796.9987,  415.9343],\n",
      "        [ 583.1193,  333.8482,  601.8462,  347.5010],\n",
      "        [ 729.9250,  328.8670,  754.0770,  387.9754],\n",
      "        [ 709.8313,  328.8125,  730.6232,  361.2158],\n",
      "        [ 310.4894,  339.4500,  375.1166,  384.7361],\n",
      "        [ 328.7814,  325.6616,  380.1366,  380.5633]])), scores: tensor([0.9969, 0.9883, 0.9700, 0.9600, 0.9585, 0.9563, 0.9100, 0.9034, 0.8433, 0.6903, 0.6607, 0.6581, 0.6553, 0.6477, 0.6251, 0.6165, 0.5815, 0.5549]), pred_classes: tensor([2, 2, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7])])}\n",
      "Image to predict: (720, 1280, 3)\n",
      "Predictions: {'instances': Instances(num_instances=17, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[1031.0050,  251.5735, 1079.8882,  374.2979],\n",
      "        [ 413.1424,  274.8765,  640.5114,  454.5294],\n",
      "        [1017.7169,  260.6067, 1039.5811,  353.1397],\n",
      "        [ 637.4901,  285.9472,  782.5434,  384.8175],\n",
      "        [ 187.7312,  261.3994,  362.4115,  387.8179],\n",
      "        [ 790.8980,  283.6037,  823.9142,  382.4926],\n",
      "        [ 880.0745,  268.9443,  910.0089,  349.2596],\n",
      "        [ 372.3962,  295.6071,  428.4373,  335.8455],\n",
      "        [   2.8308,  261.4152,  171.0702,  354.0224],\n",
      "        [   0.0000,  270.6595,  242.9950,  487.9153],\n",
      "        [ 836.7429,  275.5311,  859.6194,  342.9996],\n",
      "        [ 346.1375,  285.3531,  381.3623,  318.3953],\n",
      "        [1079.5341,  298.2739, 1107.6993,  346.3347],\n",
      "        [1044.2319,  148.3568, 1095.8907,  188.9528],\n",
      "        [ 138.6363,  282.1074,  193.9160,  337.8996],\n",
      "        [ 990.4246,  344.6618, 1032.9034,  397.6423],\n",
      "        [  97.5675,  260.5338,  361.4036,  479.7081]])), scores: tensor([0.9963, 0.9949, 0.9878, 0.9838, 0.9728, 0.9709, 0.9557, 0.9345, 0.9273, 0.9172, 0.8805, 0.7742, 0.7297, 0.6700, 0.6308, 0.5824, 0.5648]), pred_classes: tensor([ 0,  2,  0,  2,  2,  0,  0,  2,  2,  2,  0,  2, 28,  9,  2, 28,  2])])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# WARNING ⚠️ imgsz=[720, 1280] must be multiple of max stride 64, updating to [768, 1280]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_measurements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_default_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m720\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextended_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_EXAMPLES_TO_SHOW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mims\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KE7/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/measurements/ObjectDetection.py:97\u001b[0m, in \u001b[0;36mObjectDetectionMeasurements.iter_measurements\u001b[0;34m(self, bbox_offset, class_metrics, extended_summary, debug, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m ims \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (odrs, gt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mzip\u001b[39m(prediction, y)\n\u001b[1;32m     96\u001b[0m ):  \u001b[38;5;66;03m# odr = object detection result, gt = ground truth\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     measurements: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_measurements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43modrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextended_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(measurements)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "File \u001b[0;32m~/Documents/KE7/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/measurements/ObjectDetection.py:154\u001b[0m, in \u001b[0;36mObjectDetectionMeasurements._calculate_measurements\u001b[0;34m(self, odr, gt, class_metrics, extended_summary)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_measurements\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     odr: List[ObjectDetectionResultI],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     extended_summary: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    153\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mObjectDetectionUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics_for_single_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ground_truth=[  # TODO: this should be done by the caller all the way up\u001b[39;49;00m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     res[0] for res in gt\u001b[39;49;00m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ],  # BDD GT is a tuple of (ODR, attributes, timestamp)\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43modr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextended_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KE7/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/interfaces/ObjectDetectionI.py:258\u001b[0m, in \u001b[0;36mObjectDetectionUtils.compute_metrics_for_single_img\u001b[0;34m(ground_truth, predictions, class_metrics, extended_summary, debug, image)\u001b[0m\n\u001b[1;32m    256\u001b[0m pred_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions:\n\u001b[0;32m--> 258\u001b[0m     pred_boxes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_xyxy\u001b[49m())\n\u001b[1;32m    259\u001b[0m     pred_scores\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mscore)  \u001b[38;5;66;03m# score is a float or tensor\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     pred_classes\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mcls)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_xyxy'"
     ]
    }
   ],
   "source": [
    "measurements = ObjectDetectionMeasurements(model, bdd, batch_size=BATCH_SIZE, collate_fn=lambda x: x) # hacky way to avoid RuntimeError: each element in list of batch should be of equal size\n",
    "\n",
    "# WARNING ⚠️ imgsz=[720, 1280] must be multiple of max stride 64, updating to [768, 1280]\n",
    "from pprint import pprint\n",
    "for (results, ims) in islice(measurements.iter_measurements(\n",
    "        device=get_default_device(), \n",
    "        imgsz=[720, 1280],\n",
    "        bbox_offset=24,\n",
    "        debug=True,\n",
    "        conf=0.1,\n",
    "        class_metrics=True,\n",
    "        extended_summary=True,\n",
    "        ), \n",
    "    NUM_EXAMPLES_TO_SHOW):\n",
    "    pprint(results)\n",
    "    [im.show() for im in ims]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detectron2 Kernel KD2KC",
   "language": "python",
   "name": "detectron2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
