{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from scenic_reasoning.measurements.ObjectDetection import ObjectDetectionMeasurements\n",
    "from scenic_reasoning.utilities.common import get_default_device\n",
    "import torch\n",
    "from ultralytics.data.augment import LetterBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_TO_SHOW = 1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_transform = LetterBox(new_shape=(720, 1280))\n",
    "def transform_image_for_yolo(image : torch.Tensor):\n",
    "    # 1) convert from tensor to cv2 image\n",
    "    image_np  = image.permute(1, 2, 0).numpy()\n",
    "    # 2) resize to 768x1280\n",
    "    image_np = shape_transform(image=image_np)\n",
    "    # 3) convert back to tensor\n",
    "    image = torch.tensor(image_np).permute(2, 0, 1)\n",
    "    # 4) normalize to 0-1\n",
    "    image = image.to(torch.float32) / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.data.ImageLoader import Bdd100kDataset\n",
    "\n",
    "'''\n",
    "This 'transform_image_for_yolo' was isolated to be problematic with Bdd100kDataset,\n",
    "which is understandable give this transform fn was originally intended for YOLO format\n",
    "We take the original 720x1280 image input resolution to do proper inference upon\n",
    "'''\n",
    "'''\n",
    "bdd = Bdd100kDataset(\n",
    "    split=\"val\", \n",
    "    # YOLO requires images to be 640x640 or 768x1280, \n",
    "    # but BDD100K images are 720x1280 so we need to resize\n",
    "    transform=transform_image_for_yolo,  \n",
    "    use_original_categories=False,\n",
    "    use_extended_annotations=False,\n",
    ")\n",
    "'''\n",
    "\n",
    "bdd = Bdd100kDataset(\n",
    "    split=\"val\",\n",
    "    use_original_categories=False,\n",
    "    use_extended_annotations=False)\n",
    "\n",
    "print(bdd)\n",
    "print(len(bdd))\n",
    "first_img = bdd[0]\n",
    "first_img_tensor = first_img['image']\n",
    "print(first_img_tensor.shape)\n",
    "print(len(first_img['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.models.UltralyticsYolo import Yolo\n",
    "\n",
    "# https://docs.ultralytics.com/models/yolov5/#performance-metrics\n",
    "# model = Yolo(model=\"../yolov5x6u.pt\") # v5 can handle 1280 while v8 can handle 640. makes no sense ><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.models.Detectron import Detectron2Model\n",
    "\n",
    "threshold = 0.5\n",
    "config_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "weights_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "model = Detectron2Model(\n",
    "    config_file=config_file, \n",
    "    weights_file=weights_file, \n",
    "    threshold=threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "# Tested with local raw image path for Detectron2 inference upon\n",
    "sample_img_path = \"/Users/kevinchon/Documents/KE7/scenic-reasoning/data/bdd100k/images/100k/val/b1c9c847-3bda4659.jpg\"\n",
    "sample_img = Image.open(sample_img_path)\n",
    "sample_img_tensor = decode_image(sample_img_path)\n",
    "print(f\"sample_img_tensor dims {sample_img_tensor.shape}\")\n",
    "model.identify_for_image(sample_img_tensor)\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "# Tested with the specific individual image of BDD Dataset\n",
    "# Able to output of proper confidence scores and classification labels\n",
    "first_img = bdd[0]\n",
    "first_img_tensor = first_img['image']\n",
    "print(first_img_tensor.shape)\n",
    "\n",
    "model.identify_for_image(first_img_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = ObjectDetectionMeasurements(model, bdd, batch_size=BATCH_SIZE, collate_fn=lambda x: x) # hacky way to avoid RuntimeError: each element in list of batch should be of equal size\n",
    "\n",
    "# WARNING ⚠️ imgsz=[720, 1280] must be multiple of max stride 64, updating to [768, 1280]\n",
    "from pprint import pprint\n",
    "for (results, ims) in islice(measurements.iter_measurements(\n",
    "        device=get_default_device(), \n",
    "        imgsz=[720, 1280],\n",
    "        bbox_offset=24,\n",
    "        debug=True,\n",
    "        conf=0.1,\n",
    "        class_metrics=True,\n",
    "        extended_summary=True,\n",
    "        ), \n",
    "    NUM_EXAMPLES_TO_SHOW):\n",
    "    pprint(results)\n",
    "    [im.show() for im in ims]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detectron2 Kernel KD2KC",
   "language": "python",
   "name": "detectron2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
