{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenic_reasoning.data.ImageLoader import Bdd100kDataset\n",
    "from scenic_reasoning.models.UltralyticsYolo import Yolo_seg\n",
    "from scenic_reasoning.measurements.InstanceSegmentation import InstanceSegmentationMeasurements\n",
    "\n",
    "from scenic_reasoning.utilities.common import get_default_device\n",
    "import torch\n",
    "from itertools import islice\n",
    "from ultralytics.data.augment import LetterBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_transform = LetterBox(new_shape=(768, 1280))\n",
    "def transform_image_for_yolo(image : torch.Tensor):\n",
    "    # 1) convert from tensor to cv2 image\n",
    "    image_np  = image.permute(1, 2, 0).numpy()\n",
    "    # 2) resize to 768x1280\n",
    "    image_np = shape_transform(image=image_np)\n",
    "    # 3) convert back to tensor\n",
    "    image = torch.tensor(image_np).permute(2, 0, 1)\n",
    "    # 4) normalize to 0-1\n",
    "    image = image.to(torch.float32) / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_TO_SHOW = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "bdd = Bdd100kDataset(\n",
    "    split=\"val\", \n",
    "    # YOLO requires images to be 640x640 or 768x1280, \n",
    "    # but BDD100K images are 720x1280 so we need to resize\n",
    "    transform=transform_image_for_yolo,  \n",
    "    use_original_categories=False,\n",
    "    use_extended_annotations=False,\n",
    ")\n",
    "\n",
    "\n",
    "# https://docs.ultralytics.com/models/yolov5/#performance-metrics\n",
    "model = Yolo_seg(model=\"yolo11n-seg.pt\") # v5 can handle 1280 while v8 can handle 640. makes no sense ><\n",
    "measurements = InstanceSegmentationMeasurements(model, bdd, batch_size=BATCH_SIZE, collate_fn=lambda x: x) # hacky way to avoid RuntimeError: each element in list of batch should be of equal size\n",
    "# # model.identify_for_image('../demo/demo.jpg')\n",
    "# result = model._model.predict(source=['../demo/demo.jpg', '../demo/demo2.jpg'])[0]\n",
    "# print(len(result))\n",
    "# print(result.masks.data.shape)\n",
    "# first_mask = result.masks.data[0].numpy()\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.title(\"First Mask\")\n",
    "# plt.imshow(first_mask, cmap=\"gray\")  # Use grayscale for binary masks\n",
    "# plt.axis(\"off\")  # Remove axes for better visualization\n",
    "# plt.show()\n",
    "\n",
    "# print(result.masks.shape)\n",
    "# print(result.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 768x1280 15 cars, 1 truck, 1 traffic light, 182.3ms\n",
      "Speed: 0.9ms preprocess, 182.3ms inference, 11.7ms postprocess per image at shape (1, 3, 768, 1280)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "torch.Size([1, 1280])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# WARNING ⚠️ imgsz=[720, 1280] must be multiple of max stride 64, updating to [768, 1280]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (results, ims) \u001b[38;5;129;01min\u001b[39;00m islice(measurements\u001b[38;5;241m.\u001b[39miter_measurements(\n\u001b[1;32m      4\u001b[0m         device\u001b[38;5;241m=\u001b[39mget_default_device(), \n\u001b[1;32m      5\u001b[0m         imgsz\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1280\u001b[39m],\n\u001b[1;32m      6\u001b[0m         bbox_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m      7\u001b[0m         debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m         conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m         class_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         extended_summary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         ), \n\u001b[1;32m     12\u001b[0m     NUM_EXAMPLES_TO_SHOW):\n\u001b[1;32m     13\u001b[0m     pprint(results)\n\u001b[1;32m     14\u001b[0m     [im\u001b[38;5;241m.\u001b[39mshow() \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m ims]\n",
      "File \u001b[0;32m~/Desktop/Nothing/sky/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/measurements/InstanceSegmentation.py:89\u001b[0m, in \u001b[0;36mInstanceSegmentationMeasurements.iter_measurements\u001b[0;34m(self, bbox_offset, class_metrics, extended_summary, debug, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mget_default_device())\n\u001b[0;32m---> 89\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentify_for_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/Nothing/sky/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/models/UltralyticsYolo.py:259\u001b[0m, in \u001b[0;36mYolo_seg.identify_for_image\u001b[0;34m(self, image, debug, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mask_tensor\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    257\u001b[0m         mask_tensor \u001b[38;5;241m=\u001b[39m mask_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[43mInstanceSegmentationResultI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instance_count\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_hw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMask_Format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITMASK\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     instances\u001b[38;5;241m.\u001b[39mappend(instance)\n\u001b[1;32m    269\u001b[0m all_instances\u001b[38;5;241m.\u001b[39mappend(instances)\n",
      "File \u001b[0;32m~/Desktop/Nothing/sky/scenic-reasoning/scenic_reasoning/src/scenic_reasoning/interfaces/InstanceSegmentationI.py:57\u001b[0m, in \u001b[0;36mInstanceSegmentationResultI.__init__\u001b[0;34m(self, score, cls, label, instance_id, mask, image_hw, mask_format)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Initialize mask based on format\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask_format \u001b[38;5;241m==\u001b[39m Mask_Format\u001b[38;5;241m.\u001b[39mBITMASK:\n\u001b[0;32m---> 57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bitmask \u001b[38;5;241m=\u001b[39m \u001b[43mBitMasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mask_format \u001b[38;5;241m==\u001b[39m Mask_Format\u001b[38;5;241m.\u001b[39mPOLYGON:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bitmask \u001b[38;5;241m=\u001b[39m BitMasks(\n\u001b[1;32m     60\u001b[0m             polygons_to_bitmask(mask, image_hw[\u001b[38;5;241m0\u001b[39m], image_hw[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     61\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Nothing/sky/scenic-reasoning/install/detectron2/detectron2/structures/masks.py:106\u001b[0m, in \u001b[0;36mBitMasks.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(tensor, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, tensor\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor \u001b[38;5;241m=\u001b[39m tensor\n",
      "\u001b[0;31mAssertionError\u001b[0m: torch.Size([1, 1280])"
     ]
    }
   ],
   "source": [
    "\n",
    "# WARNING ⚠️ imgsz=[720, 1280] must be multiple of max stride 64, updating to [768, 1280]\n",
    "from pprint import pprint\n",
    "for (results, ims) in islice(measurements.iter_measurements(\n",
    "        device=get_default_device(), \n",
    "        imgsz=[768, 1280],\n",
    "        bbox_offset=24,\n",
    "        debug=True,\n",
    "        conf=0.1,\n",
    "        class_metrics=True,\n",
    "        extended_summary=True,\n",
    "        ), \n",
    "    NUM_EXAMPLES_TO_SHOW):\n",
    "    pprint(results)\n",
    "    [im.show() for im in ims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
