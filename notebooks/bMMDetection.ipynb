{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b51b3f-71ad-48e1-8461-46bc8010f856",
   "metadata": {},
   "source": [
    "# **MMDetection** Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b8c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jdiazchao/Git/scenic-reasoning/scenic_reasoning/src',\n",
       " '/Users/jdiazchao/Git/scenic-reasoning/scenic_reasoning/src',\n",
       " '/Users/jdiazchao/Git/scenic-reasoning/scenic_reasoning/src',\n",
       " '/Users/jdiazchao/Git/scenic-reasoning/scenic_reasoning/src',\n",
       " '/Users/jdiazchao/Git/scenic-reasoning/scenic_reasoning/src',\n",
       " '/opt/miniconda3/envs/openmmlab/lib/python38.zip',\n",
       " '/opt/miniconda3/envs/openmmlab/lib/python3.8',\n",
       " '/opt/miniconda3/envs/openmmlab/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/opt/miniconda3/envs/openmmlab/lib/python3.8/site-packages']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, pathlib\n",
    "\n",
    "root = pathlib.Path().resolve().parent\n",
    "src  = root / 'scenic_reasoning' / 'src'\n",
    "sys.path.insert(0, str(src))\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8b255e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scenic_reasoning.src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscenic_reasoning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscenic_reasoning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbMMDetection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MMDetection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scenic_reasoning.src'"
     ]
    }
   ],
   "source": [
    "from scenic_reasoning.src.scenic_reasoning.models.bMMDetection import MMDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d928e8ee-0c66-4fc6-9179-f0dc3ee8e144",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "coco_labels = {\n",
    "    -1: \"undefined\",\n",
    "    0: \"person\",\n",
    "    1: \"bicycle\",\n",
    "    2: \"car\",\n",
    "    3: \"motorcycle\",\n",
    "    4: \"airplane\",\n",
    "    5: \"bus\",\n",
    "    6: \"train\",\n",
    "    7: \"truck\",\n",
    "    8: \"boat\",\n",
    "    9: \"traffic light\",\n",
    "    10: \"fire hydrant\",\n",
    "    11: \"stop sign\",\n",
    "    12: \"parking meter\",\n",
    "    13: \"bench\",\n",
    "    14: \"bird\",\n",
    "    15: \"cat\",\n",
    "    16: \"dog\",\n",
    "    17: \"horse\",\n",
    "    18: \"sheep\",\n",
    "    19: \"cow\",\n",
    "    20: \"elephant\",\n",
    "    21: \"bear\",\n",
    "    22: \"zebra\",\n",
    "    23: \"giraffe\",\n",
    "    24: \"backpack\",\n",
    "    25: \"umbrella\",\n",
    "    26: \"handbag\",\n",
    "    27: \"tie\",\n",
    "    28: \"suitcase\",\n",
    "    29: \"frisbee\",\n",
    "    30: \"skis\",\n",
    "    31: \"snowboard\",\n",
    "    32: \"sports ball\",\n",
    "    33: \"kite\",\n",
    "    34: \"baseball bat\",\n",
    "    35: \"baseball glove\",\n",
    "    36: \"skateboard\",\n",
    "    37: \"surfboard\",\n",
    "    38: \"tennis racket\",\n",
    "    39: \"bottle\",\n",
    "    40: \"wine glass\",\n",
    "    41: \"cup\",\n",
    "    42: \"fork\",\n",
    "    43: \"knife\",\n",
    "    44: \"spoon\",\n",
    "    45: \"bowl\",\n",
    "    46: \"banana\",\n",
    "    47: \"apple\",\n",
    "    48: \"sandwich\",\n",
    "    49: \"orange\",\n",
    "    50: \"broccoli\",\n",
    "    51: \"carrot\",\n",
    "    52: \"hot dog\",\n",
    "    53: \"pizza\",\n",
    "    54: \"donut\",\n",
    "    55: \"cake\",\n",
    "    56: \"chair\",\n",
    "    57: \"couch\",\n",
    "    58: \"potted plant\",\n",
    "    59: \"bed\",\n",
    "    60: \"dining table\",\n",
    "    61: \"toilet\",\n",
    "    62: \"tv\",\n",
    "    63: \"laptop\",\n",
    "    64: \"mouse\",\n",
    "    65: \"remote\",\n",
    "    66: \"keyboard\",\n",
    "    67: \"cell phone\",\n",
    "    68: \"microwave\",\n",
    "    69: \"oven\",\n",
    "    70: \"toaster\",\n",
    "    71: \"sink\",\n",
    "    72: \"refrigerator\",\n",
    "    73: \"book\",\n",
    "    74: \"clock\",\n",
    "    75: \"vase\",\n",
    "    76: \"scissors\",\n",
    "    77: \"teddy bear\",\n",
    "    78: \"hair drier\",\n",
    "    79: \"toothbrush\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66778967-0c5e-4d0f-b1c4-35f4942720c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class BBox_Format(Enum):\n",
    "    XYXY = 'xyxy'\n",
    "\n",
    "class ObjectDetectionResultI:\n",
    "    def __init__(self, score, cls, label, bbox, image_hw, bbox_format):\n",
    "        self.score = score\n",
    "        self.cls = cls\n",
    "        self.label = label\n",
    "        self.bbox = bbox\n",
    "        self.image_hw = image_hw\n",
    "        self.bbox_format = bbox_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba3dd6-a621-4a4e-a116-13f4193329a6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acc932-8a77-44aa-a240-b872327cf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# from scenic_reasoning.interfaces.ObjectDetectionI import *\n",
    "# from scenic_reasoning.utilities.coco import coco_labels\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "\n",
    "Image = Union[np.ndarray, torch.Tensor, str]\n",
    "\n",
    "\n",
    "class MMDetection: # (ObjectDetectionModelI):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "\n",
    "        # TODO: Take config_file and/or checkpoint_file as input\n",
    "\n",
    "        self.model = kwargs.get('model', 'rtmdet_tiny_8xb32-300e_coco.py')\n",
    "        self.weights = kwargs.get('weights', 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth')\n",
    "        self.inferencer = DetInferencer(\n",
    "            model=self.model,\n",
    "            weights=self.weights,\n",
    "            device=kwargs.get('device', 'cpu')\n",
    "        )\n",
    "\n",
    "        self.batch_size = kwargs.get('batch_size', 1)\n",
    "\n",
    "    def out_to_obj(self, out: dict, image_hw: Tuple[int, int]):\n",
    "        obj = []\n",
    "        for label, score, bbox in zip(out['labels'], out['scores'], out['bboxes']):\n",
    "            obj += [\n",
    "                ObjectDetectionResultI(\n",
    "                    score=score,\n",
    "                    cls=label,\n",
    "                    label=coco_labels[label],\n",
    "                    bbox=bbox,\n",
    "                    image_hw=image_hw,\n",
    "                    bbox_format=BBox_Format.XYXY\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def identify_for_image(\n",
    "        self,\n",
    "        image: Image,\n",
    "        debug: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        if isinstance(image, str):\n",
    "            image_hw = cv2.imread(image).shape[:2]\n",
    "        else:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                image = image.detach().cpu().numpy()\n",
    "\n",
    "            image = image.astype(np.uint8)\n",
    "            image_hw = image.shape[:2]\n",
    "\n",
    "        pred = self.inferencer(\n",
    "            inputs=image,\n",
    "            out_dir=kwargs.get('out_dir', ''),\n",
    "            batch_size=1\n",
    "        )['predictions'][0]\n",
    "\n",
    "        return [self.out_to_obj(pred, image_hw)]\n",
    "\n",
    "    def identify_for_image_batch(\n",
    "        self,\n",
    "        images: Union[List[Image], str],\n",
    "        debug: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        image_hws = []\n",
    "        input_data = images\n",
    "\n",
    "        if isinstance(images, str):\n",
    "            image_dir = Path(images)\n",
    "            \n",
    "            image_paths = sorted([\n",
    "                p for p in image_dir.iterdir() \n",
    "                if p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']\n",
    "            ])\n",
    "            if not image_paths:\n",
    "                return []\n",
    "            \n",
    "            for path in image_paths:\n",
    "                image_hws.append(cv2.imread(str(path)).shape[:2])\n",
    "            \n",
    "            input_data = [str(p) for p in image_paths]\n",
    "\n",
    "        elif isinstance(images, list):\n",
    "            if not images:\n",
    "                return []\n",
    "            \n",
    "            processed_list = []\n",
    "            for img_item in images:\n",
    "                if isinstance(img_item, str):\n",
    "                    image_hw = cv2.imread(img_item).shape[:2]\n",
    "                    processed_list.append(img_item)\n",
    "                elif isinstance(img_item, torch.Tensor):\n",
    "                    img_np = img_item.detach().cpu().numpy().astype(np.uint8)\n",
    "                    image_hw = img_np.shape[:2]\n",
    "                    processed_list.append(img_np)\n",
    "                elif isinstance(img_item, np.ndarray):\n",
    "                    img_np = img_item.astype(np.uint8)\n",
    "                    image_hw = img_np.shape[:2]\n",
    "                    processed_list.append(img_np)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unsupported image type in list: {type(img_item)}\")\n",
    "                image_hws.append(image_hw)\n",
    "            input_data = processed_list\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a list of images or a path to a directory.\")\n",
    "\n",
    "        predictions = self.inferencer(\n",
    "            inputs=input_data,\n",
    "            out_dir=kwargs.get('out_dir', ''),\n",
    "            batch_size=kwargs.get('batch_size', self.batch_size)\n",
    "        )['predictions']\n",
    "\n",
    "        return [self.out_to_obj(pred, hw) for pred, hw in zip(predictions, image_hws)]\n",
    "\n",
    "\n",
    "    def to(self, device: Union[str, torch.device]):\n",
    "        self.inferencer = DetInferencer(\n",
    "            model=self.model,\n",
    "            weights=self.weights,\n",
    "            device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6441a54-8c8a-4ce8-95ce-64cc6ae72e07",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188017c-a45e-4a75-9e7f-83cba786305a",
   "metadata": {},
   "source": [
    "### Download models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3986cc-5f77-462f-b638-64e54f440c07",
   "metadata": {},
   "source": [
    "With openmim installed, run the following to download the model\n",
    "    \n",
    "    mim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc145b-2069-416e-a7ab-1a50a98dfbdc",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4aba41e-2832-4591-be7e-e9a69615f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MMDetection(\n",
    "    model='rtmdet_tiny_8xb32-300e_coco.py',\n",
    "    weights='rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceda8f4-240c-4ddd-af07-c0f2256a19ef",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bd1b11-0384-4a71-8516-c2e3fadda9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(results: List[List[ObjectDetectionResultI]]):\n",
    "    for idx, i in enumerate(results[:2]):\n",
    "        print(f\"Image {idx+1}\")\n",
    "        print(f\"    Detected {len(i)} objects\")\n",
    "        for j in i[:4]:\n",
    "            print(f\"        label={j.label}, bbox={j.bbox}, score={j.score}\")\n",
    "        if len(i) > 4:\n",
    "            print(\"        ...\")\n",
    "    if len(results) > 2:\n",
    "        print(\"    ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350f10d-0148-4812-a24e-0a81ec4b35e8",
   "metadata": {},
   "source": [
    "Support for single image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c828939c-0d9b-47ae-ad55-775a76aa3436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n"
     ]
    }
   ],
   "source": [
    "# str\n",
    "eval(model.identify_for_image('demo/demo.jpg'))\n",
    "\n",
    "# numpy.ndarray\n",
    "import cv2\n",
    "np_im = cv2.imread(\"demo/demo.jpg\")\n",
    "eval(model.identify_for_image(np_im))\n",
    "\n",
    "# torch.Tensor\n",
    "torch_im = torch.from_numpy(np_im)\n",
    "eval(model.identify_for_image(torch_im))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9c52c-6fd2-43af-8837-fa69c2a156d2",
   "metadata": {},
   "source": [
    "and batch image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079b0304-f06e-4bdc-aeef-95fe0e6bd4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n",
      "Image 2\n",
      "    Detected 300 objects\n",
      "        label=person, bbox=[2472.887939453125, 298.7748718261719, 2928.625732421875, 1002.2025756835938], score=0.7596670389175415\n",
      "        label=person, bbox=[1560.3583984375, 193.0028076171875, 1881.524169921875, 754.938232421875], score=0.7277154922485352\n",
      "        label=person, bbox=[1651.1370849609375, 362.84014892578125, 2243.866943359375, 986.57421875], score=0.7058905363082886\n",
      "        label=person, bbox=[2106.547607421875, 241.47494506835938, 2495.137939453125, 764.114990234375], score=0.7028319239616394\n",
      "        ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n",
      "Image 2\n",
      "    Detected 300 objects\n",
      "        label=person, bbox=[2472.887939453125, 298.7748718261719, 2928.625732421875, 1002.2025756835938], score=0.7596670389175415\n",
      "        label=person, bbox=[1560.3583984375, 193.0028076171875, 1881.524169921875, 754.938232421875], score=0.7277154922485352\n",
      "        label=person, bbox=[1651.1370849609375, 362.84014892578125, 2243.866943359375, 986.57421875], score=0.7058905363082886\n",
      "        label=person, bbox=[2106.547607421875, 241.47494506835938, 2495.137939453125, 764.114990234375], score=0.7028319239616394\n",
      "        ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n",
      "Image 2\n",
      "    Detected 300 objects\n",
      "        label=person, bbox=[2472.887939453125, 298.7748718261719, 2928.625732421875, 1002.2025756835938], score=0.7596670389175415\n",
      "        label=person, bbox=[1560.3583984375, 193.0028076171875, 1881.524169921875, 754.938232421875], score=0.7277154922485352\n",
      "        label=person, bbox=[1651.1370849609375, 362.84014892578125, 2243.866943359375, 986.57421875], score=0.7058905363082886\n",
      "        label=person, bbox=[2106.547607421875, 241.47494506835938, 2495.137939453125, 764.114990234375], score=0.7028319239616394\n",
      "        ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "    Detected 300 objects\n",
      "        label=bench, bbox=[221.37191772460938, 176.12808227539062, 456.25811767578125, 383.2401428222656], score=0.8703237771987915\n",
      "        label=car, bbox=[295.3505554199219, 117.18350219726562, 378.571533203125, 150.27117919921875], score=0.7677364945411682\n",
      "        label=car, bbox=[190.573486328125, 109.70985412597656, 299.5221252441406, 155.0396270751953], score=0.7427825331687927\n",
      "        label=car, bbox=[431.36944580078125, 104.98468017578125, 484.879150390625, 131.94033813476562], score=0.6994597911834717\n",
      "        ...\n",
      "Image 2\n",
      "    Detected 300 objects\n",
      "        label=person, bbox=[2472.887939453125, 298.7748718261719, 2928.625732421875, 1002.2025756835938], score=0.7596670389175415\n",
      "        label=person, bbox=[1560.3583984375, 193.0028076171875, 1881.524169921875, 754.938232421875], score=0.7277154922485352\n",
      "        label=person, bbox=[1651.1370849609375, 362.84014892578125, 2243.866943359375, 986.57421875], score=0.7058905363082886\n",
      "        label=person, bbox=[2106.547607421875, 241.47494506835938, 2495.137939453125, 764.114990234375], score=0.7028319239616394\n",
      "        ...\n"
     ]
    }
   ],
   "source": [
    "# str\n",
    "eval(model.identify_for_image_batch('demo', batch_size=2))\n",
    "\n",
    "# list[str]\n",
    "eval(model.identify_for_image_batch(['demo/demo.jpg', 'demo/demo2.jpg'], batch_size=2))\n",
    "\n",
    "# list[numpy.ndarray]\n",
    "np_im1 = cv2.imread(\"demo/demo.jpg\")\n",
    "np_im2 = cv2.imread(\"demo/demo2.jpg\")\n",
    "eval(model.identify_for_image_batch([np_im1, np_im2], batch_size=2))\n",
    "\n",
    "# list[str]\n",
    "torch_im1 = torch.from_numpy(np_im1)\n",
    "torch_im2 = torch.from_numpy(np_im2)\n",
    "eval(model.identify_for_image_batch([torch_im1, torch_im2], batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e4793-daa2-497f-8d6e-9932a272a768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
