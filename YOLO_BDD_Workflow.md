# BDD100K â†’ YOLOv9 Training Workflow

This short guide explains **(1) how to export BDD100K annotations to YOLO-compatible `.txt` files** and **(2) how to start a multi-GPU YOLOv9 training run logged to Weights & Biases (wandb)**.  Two launch modes are covered: *local workstation* and *Slurm cluster*.

---
## 1  Export YOLO labels

```bash
# Activate the project environment
conda activate scenic_reason

# Run the exporter (1â€“3 min per split on a V100)
python graid/src/graid/data/export_bdd_to_yolo.py
```

The script:
* Instantiates `Bdd100kDataset(split, use_original_categories=True, use_time_filtered=False)`.
* Converts each `ObjectDetectionResultI` to YOLOâ€style **normalized** `x_center y_center width height` using `as_xywhn()`.
* Writes one `.txt` per image **only when at least one object is present**.

### Output structure (expected by Ultralytics)
```
data/bdd100k/
â”œâ”€â”€ images/100k/train/   *.jpg               â† original images
â”œâ”€â”€ images/100k/val/     *.jpg
â””â”€â”€ labels/100k/
    â”œâ”€â”€ train/           *.txt               â† generated by script
    â””â”€â”€ val/             *.txt
```
If you initially generated files under `yolo_labels/`, move them into the parallel `labels/100k/{train,val}/` directories:

```bash
mkdir -p data/bdd100k/labels/100k/{train,val}
mv data/bdd100k/yolo_labels/train/*.txt data/bdd100k/labels/100k/train/
mv data/bdd100k/yolo_labels/val/*.txt   data/bdd100k/labels/100k/val/
```

> **Tip:** set `root_output_dir = Path("data/bdd100k/labels/100k")` in the script if you prefer the exporter to write directly to the correct place.

---
## 2  Start training (wandb logging)

### 2.1  Local / interactive
```bash
python train_yolo_bdd.py
```
`train_yolo_bdd.py` will:
1. Detect available GPUs via `CUDA_VISIBLE_DEVICES` (or fall back to all GPUs).
2. Set `WANDB_PROJECT=yolo_bdd`, `WANDB_NAME=yolo_bdd_train` (edit if you like).
3. Launch Ultralytics training:
   * Model  : `yolov9e.pt` (pre-trained)
   * Dataset: `bdd_ultra.yaml`
   * Epochs : 100   â€¢   Image size : 1080   â€¢   Batch : 32
4. Artifacts & logs are stored in `runs/detect/yolo_bdd/` and on wandb.

Requirements
* `wandb login` once beforehand (or set `WANDB_MODE=offline`).
* ~128 GB RAM, 4 GPUs, < 10 h wall-time (V100).

### 2.2  Slurm cluster
1. **Inspect / edit resources** in `train_yolo_bdd.slurm` (default: 4 GPUs, 16 CPU, 128 GB, 10 h).
2. Submit the job:
   ```bash
   sbatch train_yolo_bdd.slurm
   ```
3. Logs stream to `slurm-<jobid>.out` and wandb.

The batch script:
* Loads your conda env (`scenic_reason`).
* Exports `WANDB_MODE=online` (remove or change to *offline* if desired).
* Executes `python train_yolo_bdd.py`.  Ultralytics launches DDP automatically across the 4 GPUs allocated by Slurm.

---
## 3  Monitoring & artefacts
* **wandb:**
  ```bash
  wandb online            # if you disabled it earlier
  # view runs at https://wandb.ai/<entity>/yolo_bdd
  ```
* **TensorBoard:**
  ```bash
  tensorboard --logdir runs/detect/yolo_bdd
  ```
* **Best weights:** `runs/detect/yolo_bdd/weights/best.pt`

Happy training! ðŸš€ 